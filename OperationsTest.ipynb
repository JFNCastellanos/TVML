{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0cd4fba-0d61-4995-ae3d-b127608080c5",
   "metadata": {},
   "source": [
    "# Testing operations with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa6dfe3-9d75-4267-8b81-923445d3384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import parameters as var #Configuration and coarsening parameters\n",
    "var.init() #initializes parameters\n",
    "import utils as ut #Some utility functions \n",
    "import loss_function as lf #Custom loss function\n",
    "import model as model\n",
    "\n",
    "\n",
    "from opendataset import ConfsDataset #class for opening gauge confs\n",
    "from opendataset import read_binary_conf\n",
    "import operators as op #Interpolator and prolongator given a set of test vectors\n",
    "import operators_torch as opt #Same implementations but with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2b6560-154e-4f36-b1d3-0d3f089c4f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_loss(pred_test_vectors,test_vectors):\n",
    "    loss = 0\n",
    "    for i in range(batch_size):\n",
    "        ops = op.Operators(var.BLOCKS_X,var.BLOCKS_T,pred_test_vectors[i])\n",
    "        for tv in range(var.NV):\n",
    "            #Evaluate \n",
    "            loss += np.linalg.norm( (test_vectors[i,tv] - ops.P_Pdagg(test_vectors[i,tv])) )\n",
    "            #loss = np.linealg.norm(test_vectors - P_Pdagg(test_vectors)) #|| . ||₂ (l2-norm)\n",
    "    return loss\n",
    "\n",
    "def torch_loss(pred, target):\n",
    "    \"\"\"\n",
    "    pred  : Tensor of shape (B, NV, 2, NT, NX)   (complex numbers stored as complex dtype)\n",
    "    target: Tensor of shape (B, NV, 2, NT, NX)   (the “near kernel” vectors)\n",
    "    Returns a scalar loss Tensor (requires_grad=True)\n",
    "    \"\"\"\n",
    "    batch_size = pred.shape[0]\n",
    "    loss = 0.0\n",
    "    for i in range(batch_size):\n",
    "            \n",
    "        ops = opt.Operators(var.BLOCKS_X, var.BLOCKS_T, pred[i])   \n",
    "        for tv in range(var.NV):\n",
    "            corrected = ops.P_Pdagg(target[i, tv])               \n",
    "            diff = target[i, tv] - corrected\n",
    "            loss = loss + torch.linalg.norm(diff)   \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd325be-7f4e-4499-b410-edceb4ad4c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_loss_ind(pred_test_vectors,test_vectors):\n",
    "    loss = 0.0\n",
    "    ops = op.Operators(var.BLOCKS_X,var.BLOCKS_T,pred_test_vectors)\n",
    "    if test_vectors.shape[0] == var.NV:\n",
    "        for tv in range(var.NV):\n",
    "            #Evaluate \n",
    "            loss += np.linalg.norm( (test_vectors[tv] - ops.P_Pdagg(test_vectors[tv])) )/np.linalg.norm(test_vectors[tv])\n",
    "            #loss = np.linealg.norm(test_vectors - P_Pdagg(test_vectors)) #|| . ||₂ (l2-norm)\n",
    "    else:\n",
    "        loss += np.linalg.norm( (test_vectors - ops.P_Pdagg(test_vectors)) )/np.linalg.norm(test_vectors)\n",
    "    return loss\n",
    "\n",
    "def torch_loss_ind(pred, target):\n",
    "    loss = 0.0      \n",
    "    ops = opt.Operators(var.BLOCKS_X, var.BLOCKS_T, pred)\n",
    "    if target.shape[0] == var.NV:\n",
    "        for tv in range(var.NV):\n",
    "            corrected = ops.P_Pdagg(target[tv])               \n",
    "            diff = target[tv] - corrected\n",
    "            loss = loss + torch.linalg.norm(diff)/torch.linalg.norm(target[tv])\n",
    "    else:\n",
    "        loss += torch.linalg.norm( (target - ops.P_Pdagg(target)) ) / torch.linalg.norm(target)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f58947-a3e0-42fa-ac94-4f6a828a92d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loading the configurations and the near-kernel test vectors\n",
    "\"\"\"\n",
    "workers = 4\n",
    "batch_size = 300\n",
    "dataset = ConfsDataset()\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                shuffle=False, num_workers=workers)\n",
    "\n",
    "#----returns a tensor of size [ [batch_size,4,Nx,Nt], [batch_size,Nv,2,Nx,Nt] ]----#\n",
    "first_batch = next(iter(dataloader)) \n",
    "#--------------------------------------\n",
    "#first_batch[0][0].shape\n",
    "#print(\"Re(U0)\",first_batch[0][0][0,0,0])\n",
    "#print(\"Re(U1)\",first_batch[0][0][1,0,0])\n",
    "#print(\"Im(U0)\",first_batch[0][0][2,0,0])\n",
    "#print(\"Im(U1)\",first_batch[0][0][3,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0085e16b-29fc-4b49-ba14-b18649092015",
   "metadata": {},
   "outputs": [],
   "source": [
    "near_kernel = first_batch[1]\n",
    "b, tv = 0, 0\n",
    "v1 = near_kernel[b,tv]\n",
    "norm = 0\n",
    "for t in range(var.NT):\n",
    "    for x in range(var.NX):\n",
    "        for mu in range(2):\n",
    "            norm += v1[mu,t,x] * torch.conj(v1[mu,t,x])\n",
    "norm = torch.sqrt(norm)\n",
    "print(\"Norm\",norm)\n",
    "normas = torch.linalg.vector_norm(near_kernel[:,:],dim=(-3,-2,-1)).view(batch_size,var.NV,1,1,1)\n",
    "near_kernel_normalize = near_kernel/normas\n",
    "print(\"Norm with torch.linalg.vector_norm\",normas[b,tv])\n",
    "print(\"Normalized vector\",near_kernel_normalize[b,tv,0,0,0])\n",
    "print(\"Normalized vector (by hand)\",near_kernel[b,tv,0,0,0]/norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff6a6dd-960e-4fa5-b1f9-e516ec52fd0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = model.neural_net3(first_batch[0])\n",
    "B = pred.shape[0]\n",
    "print(pred.shape)\n",
    "pred = pred.view(B, var.NV, 4, var.NT, var.NX)      # (B,NV,4,NT,NX)\n",
    "print(pred.shape)\n",
    "# Build a complex tensor of shape (B, NV, 2, NT, NX)\n",
    "#   channel 0 → real part of component 0\n",
    "#   channel 1 → real part of component 1\n",
    "#   channel 2 → imag part of component 0\n",
    "#   channel 3 → imag part of component 1\n",
    "real = torch.stack([pred[:,:, 0], pred[:,:, 1]], dim=2)   # (B,NV,2,NT,NX)\n",
    "imag = torch.stack([pred[:,:, 2], pred[:,:, 3]], dim=2)   # (B,NV,2,NT,NX)\n",
    "pred_complex = torch.complex(real, imag)                  # (B,NV,2,NT,NX)\n",
    "norms = torch.linalg.vector_norm(pred_complex[:,:],dim=(-3,-2, -1)).view(batch_size, var.NV, 1, 1, 1)\n",
    "norms_broadcastable = norms.view(B, var.NV, 1, 1, 1)\n",
    "pred_complex_normalized = pred_complex / norms_broadcastable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9020a00f-cc24-4bed-aa9f-8199f8aa315d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf, tv = 78, 0\n",
    "print(torch.linalg.norm(pred_complex[conf,tv]))\n",
    "print(torch.linalg.norm(pred_complex_normalized[conf,tv]))\n",
    "print(norms_broadcastable[conf,tv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46485931-e1a1-4faa-b070-3bb8cef94ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For testing torch vs numpy\n",
    "test = first_batch[1][0]\n",
    "\n",
    "print(\"set of test vectors\",type(test))\n",
    "oper = op.Operators(var.BLOCKS_X, var.BLOCKS_T,test)\n",
    "tvecs = oper.getTestVectors()\n",
    "print(\"numpy vectors\",type(tvecs))\n",
    "oper_torch = opt.Operators(var.BLOCKS_X, var.BLOCKS_T,test)\n",
    "tvecs_torch = oper_torch.getTestVectors()\n",
    "print(\"torch vectors\",type(tvecs_torch))\n",
    "print(tvecs.shape,tvecs_torch.shape)\n",
    "print(\"test vectors\",tvecs[0,1,2,2],tvecs_torch[0,1,2,2])\n",
    "\n",
    "v = np.random.rand(2,var.NT,var.NX) + 1j*np.random.rand(2,var.NT,var.NX)\n",
    "vc = np.random.rand(var.NV,2,var.BLOCKS_T,var.BLOCKS_X) + 1j*np.random.rand(var.NV,2,var.BLOCKS_T,var.BLOCKS_X)\n",
    "\n",
    "vtorch = torch.tensor(v)\n",
    "vctorch = torch.tensor(vc)\n",
    "\n",
    "\n",
    "out = oper.P_vc(vc)\n",
    "print(\"Pvc (np)\",out[0,5,0],type(out))\n",
    "outt = oper_torch.P_vc(vctorch)\n",
    "print(\"Pvc (torch)\",outt[0,5,0],type(outt))\n",
    "\n",
    "out = oper.P_Pdagg(v)\n",
    "print(\"PP^+ (np)\",v[0,5,0],out[0,5,0])\n",
    "outvc = oper.Pdagg_P(vc)\n",
    "print(\"P^+P (np)\",vc[0,0,0,0],outvc[0,0,0,0])\n",
    "\n",
    "outt = oper_torch.P_Pdagg(vtorch)\n",
    "print(\"PP^+ (torch)\",vtorch[0,5,0],outt[0,5,0])\n",
    "outvct = oper_torch.Pdagg_P(vctorch)\n",
    "print(\"P^+P (torch)\",vctorch[0,0,0,0],outvct[0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ed47d9-f06c-42e8-8664-8e2a139c2caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(4)\n",
    "test_vectors = np.zeros((var.NV,2,var.NT,var.NX),dtype=complex)\n",
    "for tv in range(var.NV):\n",
    "    for nt in range(var.NT):\n",
    "        for nx in range(var.NX):\n",
    "            for s in range(2):\n",
    "                x = random.random()\n",
    "                y = random.random()\n",
    "                z = complex(x, y)\n",
    "                test_vectors[tv,s,nt,nx] = z   #2*(nx*NT + nt) + s + 1 + tv*2*NX*NT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb3fd38-1883-4965-83b7-edd8cc49a91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(numpy_loss_ind(first_batch[1][0].numpy(),first_batch[1][0].numpy()))\n",
    "print(torch_loss_ind(first_batch[1][0],first_batch[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c70ccae-5f1d-4f2d-9bde-0e83f2cfc41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This checks whether random test vectors are contained on the image of P when this operator is \n",
    "#assembled with the test vectors from SAP\n",
    "print(numpy_loss_ind(first_batch[1][0].numpy(),test_vectors)/var.NV)\n",
    "print(torch_loss_ind(first_batch[1][0],torch.tensor(test_vectors))/var.NV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ff733b-0cc5-4938-8397-83b751991b02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#This does the opposite: checks whether the test vectors from SAP are contained in the image of P when built\n",
    "#with random vectors\n",
    "print(numpy_loss_ind(test_vectors,first_batch[1][0].numpy())/var.NV)\n",
    "print(torch_loss_ind(torch.tensor(test_vectors),first_batch[1][2])/var.NV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6ddef4-e82f-46cb-8694-c4df4241d9ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Checking that the test vectors generated with SAP are approximately contained in the range of P\n",
    "loss = 0.0\n",
    "remTV = 30-var.NV\n",
    "for confID in range(batch_size):\n",
    "    print(\"confID\",confID)\n",
    "    for i in range(remTV):\n",
    "        tvector = np.zeros((2,var.NT,var.NX),dtype=complex)\n",
    "        path = 'sap/near_kernel/b{0}_{1}x{2}/{3}/tvector_{1}x{2}_b{0}0000_m{4}_nconf{5}_tv{6}.tv'.format(\n",
    "        int(var.BETA),var.NX,var.NT,var.M0_FOLDER,var.M0_STRING,confID,29-i)\n",
    "        tvector = read_binary_conf(None,path)\n",
    "        print(\"Test vector \",29-i)\n",
    "        loss += numpy_loss_ind(first_batch[1][confID].numpy(),tvector)\n",
    "        print(\"Numpy\",numpy_loss_ind(first_batch[1][confID].numpy(),tvector))\n",
    "        print(\"PyTorch\",torch_loss_ind(first_batch[1][confID],torch.tensor(tvector)))\n",
    "    print(\"----------------------\")\n",
    "print(\"Final loss on the remainder test vectors\",loss/(batch_size*remTV))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa41169-9a3a-49c7-b703-af8d33cdcf2d",
   "metadata": {},
   "source": [
    "Final loss on the remainder test vectors 9.386980374900652 (for 4 test vectors and using the rest to check the loss function)\n",
    "Final loss on the remainder test vectors 2.5979268393039137 (for 14 test vectors and using the rest to check the loss function)\n",
    "\n",
    "This shows that the more smoothed test vectors, the better they capture other near-kernel components In other terms, the more smoothed test vectors I use, the smaller the metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65273607-c0f8-46b7-94ce-a74125184763",
   "metadata": {},
   "source": [
    "Just out of curiosity. What happens if I evaluate the metric just on random vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24ca2df-261e-481d-abbe-6c7bf900a6d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Using the test vectors from SAP to build P, P^+ and evaluate on random test vectors.\n",
    "loss = 0.0\n",
    "for confID in range(batch_size):\n",
    "    print(\"confID\",confID)\n",
    "    for i in range(var.NV):\n",
    "        tvector = np.random.rand(2,var.NT,var.NX) + 1j*np.random.rand(2,var.NT,var.NX)\n",
    "        print(\"rand vector\",i)\n",
    "        loss += numpy_loss_ind(first_batch[1][confID].numpy(),tvector)\n",
    "        print(\"Numpy\",numpy_loss_ind(first_batch[1][confID].numpy(),tvector))\n",
    "        print(\"PyTorch\",torch_loss_ind(first_batch[1][confID],torch.tensor(tvector)))\n",
    "    print(\"----------------------\")\n",
    "print(\"Final loss on the remainder test vectors\",loss/(batch_size*var.NV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a083c08f-bd42-406f-b8e2-58db1382e9c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Checking how well random vectors contain the SAP test vectors\n",
    "test_vectors = np.zeros((var.NV,2,var.NT,var.NX),dtype=complex)\n",
    "#random.seed(0)\n",
    "for tv in range(var.NV):\n",
    "    for nt in range(var.NT):\n",
    "        for nx in range(var.NX):\n",
    "            for s in range(2):\n",
    "                x = random.random()\n",
    "                y = random.random()\n",
    "                z = complex(x, y)\n",
    "                test_vectors[tv,s,nt,nx] = z   #2*(nx*NT + nt) + s + 1 + tv*2*NX*NT\n",
    "\n",
    "loss = 0.0\n",
    "for confID in range(batch_size):\n",
    "    print(\"confID\",confID)\n",
    "    loss += numpy_loss_ind(test_vectors,first_batch[1][confID].numpy())\n",
    "    print(\"Numpy\",numpy_loss_ind(test_vectors,first_batch[1][confID].numpy()))\n",
    "    print(\"PyTorch\",torch_loss_ind(torch.tensor(test_vectors),first_batch[1][confID]))\n",
    "    print(\"----------------------\")\n",
    "print(\"Final loss on the remainder test vectors\",loss/(batch_size*var.NV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30714f64-4ef4-48c9-985e-c6404844bc96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
